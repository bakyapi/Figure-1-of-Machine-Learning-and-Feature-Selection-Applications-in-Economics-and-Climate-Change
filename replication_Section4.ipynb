{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1dff8a-7b69-45b4-871c-d07276030b81",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <center>\n",
    "    Machine Learning and Feature Selection: Applications in Economics and Climate Change\n",
    "        </center>\n",
    "    </h2>\n",
    "    \n",
    "<h3>\n",
    "    <center>\n",
    "Berkay Akyapi\n",
    "        </center>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0ec63-3bb1-40f8-b939-0ecd5f26b883",
   "metadata": {},
   "source": [
    "**Replication of the Application presented in Section 4**\n",
    "\n",
    "This Jupyter notebook replicates the application presented in Section 4 of the paper. It consists of the following steps:\n",
    "\n",
    "1. Downloading US county-level daily temperatures between 1979 - 2019 from ERA5.\n",
    "2. Generating county-level limits for different definitions of heatwaves.\n",
    "3. Generating heatwave information.\n",
    "4. Performing feature selection using Group LASSO, Sparse Group LASSO, and LASSO.\n",
    "5. Presenting regression results and generating Figure 4.\n",
    "\n",
    "**Note on Dependencies and Environments**\n",
    "\n",
    "- Some required packages are imported as needed during the execution.\n",
    "- To prevent potential inconsistencies with the \"asgl\" package and its dependencies (especially with linearmodels), it is suggested to download that package and its dependencies in a separate Python environment. Run the relevant code for Group LASSO and Sparse Group LASSO in that environment.\n",
    "- Files are saved after each section, so there is no need to repeat those sections again.\n",
    "- The initial part until heatwave generation can take a while to complete.\n",
    "- Adjust paths to save/read files as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41933a66-0641-4438-bbca-8ecfbb4d1e5e",
   "metadata": {},
   "source": [
    "# Downloading Daily Temperature Data for US Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8231f0-1803-4c41-b468-f71699daeb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import os\n",
    "\n",
    "print('### Geemap version: ' + geemap.__version__) # 0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7ccb2-88ad-4448-8981-5e810a7db71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to print in bold text (or other colors if needed)\n",
    "class myColor:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3ff48-217d-4a67-8b0f-d0cfdedaae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parentPath = os.path.split(os.path.abspath(os.getcwd()))[0]\n",
    "print(parentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7fe94-d500-430e-9cf6-ca3652f0c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bff82-8cb4-4dc0-93d2-7640bc9e79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# county boundaries\n",
    "counties = ee.FeatureCollection(\"TIGER/2018/Counties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446b634-d9b4-4de7-a30b-4b4dd58d6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(parentPath, 'Data', 'US_County_T') # Adjust the address accordingly\n",
    "year = 1979\n",
    "for i in range(2021-year):\n",
    "    for j in range(1,13):\n",
    "        if j ==12:\n",
    "            theDateBegin = str(year) + '-' + str(j) + '-01'\n",
    "            theDateEnd = str(year+1) + '-01-01'\n",
    "        else:\n",
    "            theDateBegin = str(year) + '-' + str(j) + '-01'\n",
    "            theDateEnd = str(year) + '-' + str(j+1) + '-01'\n",
    "        print('')\n",
    "        print('Dates are ', theDateBegin, 'to', theDateEnd)\n",
    "        print('')\n",
    "        \n",
    "        # Temperature Data (Mean)\n",
    "        docName = 'meanDailyTemp' + str(year) + '_' + str(j) + '.csv'\n",
    "        print('Document Name is ', myColor.BOLD + docName + myColor.END)\n",
    "        print('')\n",
    "        era5_2mt = ee.ImageCollection('ECMWF/ERA5/DAILY').filter(ee.Filter.date(theDateBegin, theDateEnd)).select('mean_2m_air_temperature')\n",
    "        out_landstat_stats = os.path.join(out_dir, docName)\n",
    "        geemap.zonal_statistics(era5_2mt, counties, out_landstat_stats, statistics_type='MEAN', scale = 5000)\n",
    "        \n",
    "        # Temperature Data (Min)\n",
    "        docName = 'minDailyTemp' + str(year) + '_' + str(j) + '.csv'\n",
    "        print('')\n",
    "        print('Document Name is ', myColor.BOLD + docName + myColor.END)\n",
    "        print('')\n",
    "        era5_2mt = ee.ImageCollection('ECMWF/ERA5/DAILY').filter(ee.Filter.date(theDateBegin, theDateEnd)).select('minimum_2m_air_temperature')\n",
    "        out_landstat_stats = os.path.join(out_dir, docName)\n",
    "        geemap.zonal_statistics(era5_2mt, counties, out_landstat_stats, statistics_type='MEAN', scale = 5000)\n",
    "        \n",
    "        # Temperature Data (Max)\n",
    "        docName = 'maxDailyTemp' + str(year) + '_' + str(j) + '.csv'\n",
    "        print('')\n",
    "        print('Document Name is ', myColor.BOLD + docName + myColor.END)\n",
    "        print('')\n",
    "        era5_2mt = ee.ImageCollection('ECMWF/ERA5/DAILY').filter(ee.Filter.date(theDateBegin, theDateEnd)).select('maximum_2m_air_temperature')\n",
    "        out_landstat_stats = os.path.join(out_dir, docName)\n",
    "        geemap.zonal_statistics(era5_2mt, counties, out_landstat_stats, statistics_type='MEAN', scale = 5000)\n",
    "    \n",
    "    year = year + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef114614-efc7-4387-96d4-31022537e3e1",
   "metadata": {},
   "source": [
    "# Reading and Merging Daily Temperature Data and Calculation of Heatwaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5564fc-a834-4faa-b793-6962d267ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.matlib\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# from linearmodels.panel import FirstDifferenceOLS\n",
    "# from linearmodels.panel import PanelOLS\n",
    "\n",
    "# Check software versions\n",
    "print('### Python version: ' + __import__('sys').version) # 3.9.12\n",
    "print('### NumPy version: ' + np.__version__) # 1.22.3\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e3ed6-eee9-47c4-aa64-4266c44d7a12",
   "metadata": {},
   "source": [
    "**Function to Read the Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758f1de-d4f1-43a6-b050-98a294231602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(dummyData, months, theDir, docName, theTemp, colInd, varname):\n",
    "\n",
    "    theData = pd.DataFrame(index=None)\n",
    "    theDataFebruary29 = pd.DataFrame()\n",
    "    \n",
    "    year = 1979\n",
    "    for m in range(2020-year):\n",
    "        # More than 35C\n",
    "        myData2 = pd.DataFrame(index = None)\n",
    "        myData2['County'] = dummyData['NAME'].values\n",
    "        myData2['CountyFP'] = dummyData['COUNTYFP'].values\n",
    "        myData2['StateFP'] = dummyData['STATEFP'].values\n",
    "\n",
    "\n",
    "        for i, myMo in enumerate(months):\n",
    "\n",
    "            if year < 2020 or i < 6: # No Data after June for 2020\n",
    "                docNameMore = docName + str(year) + '_' + str(i+1) + '.csv'\n",
    "                myData1 = pd.read_csv(theDir + docNameMore)\n",
    "                myData1 = myData1.drop(['GEOID', 'LSAD', 'CBSAFP', 'CSAFP', 'FUNCSTAT', 'INTPTLAT',\n",
    "                                       'INTPTLON', 'COUNTYNS', 'MTFCC', 'system:index', 'METDIVFP', 'NAMELSAD',\n",
    "                                       'METDIVFP', 'AWATER', 'ALAND','COUNTYFP', 'STATEFP', 'CLASSFP', 'NAME'], axis=1)\n",
    "\n",
    "                if m == 0 and i == 0: # No observation for January 1, 1979: Replace with mean\n",
    "                    myData1.insert(0, (str(year) + '0101_' + theTemp + varname), \n",
    "                                   myData1.copy().mean(axis=1))\n",
    "                \n",
    "                if i+1 == 2:\n",
    "                    colNamesMore = [myMo + str(j) + colInd for j in range(1,29)]\n",
    "                    if year == 1980 or year == 1984 or year == 1988 or year == 1992 or year == 1996 or year == 2000 or year == 2004 or year == 2008 or year == 2012 or year == 2016 or year == 2020:\n",
    "\n",
    "                        myString = str(year) + '0229_' + theTemp + varname\n",
    "                        theMore = pd.DataFrame()\n",
    "                        theMore['Feb29' + colInd] = myData1[myString]\n",
    "                        theMore['Year'] = year\n",
    "                        theMore['County'] = dummyData['NAME'].values\n",
    "                        theDataFebruary29 = pd.concat([theDataFebruary29, theMore], axis = 0)\n",
    "                        myData1 = myData1.drop(myString, axis=1) \n",
    "\n",
    "                elif i+1 == 4 or i+1 == 6 or i+1 == 9 or i+1 == 11:\n",
    "                    colNamesMore = [myMo + str(j) + colInd for j in range(1,31)]\n",
    "                else:  \n",
    "                    colNamesMore = [myMo + str(j) + colInd for j in range(1,32)]\n",
    "\n",
    "\n",
    "                #colNamesMore.append('NAME')\n",
    "                myData1.columns = colNamesMore\n",
    "                myData2 = pd.concat([myData2, myData1], axis=1)\n",
    "\n",
    "            else:\n",
    "                docNameMore = docName + str(year-1) + '_' + str(i+1) + '.csv'\n",
    "                myData1 = pd.read_csv(theDir + docNameMore)\n",
    "                myData1 = myData1.drop(['GEOID', 'LSAD', 'CBSAFP', 'CSAFP', 'FUNCSTAT', 'INTPTLAT',\n",
    "                                       'INTPTLON', 'COUNTYNS', 'MTFCC', 'system:index', 'METDIVFP', 'NAMELSAD',\n",
    "                                       'METDIVFP', 'AWATER', 'ALAND','COUNTYFP', 'STATEFP', 'CLASSFP', 'NAME'], axis=1)\n",
    "\n",
    "\n",
    "                if i+1 == 4 or i+1 == 6 or i+1 == 9 or i+1 == 11:\n",
    "                    colNamesMore = [myMo + str(j) + colInd for j in range(1,31)]\n",
    "                else:  \n",
    "                    colNamesMore = [myMo + str(j) + colInd for j in range(1,32)]\n",
    "\n",
    "\n",
    "                #colNamesMore.append('NAME')\n",
    "                myData1.columns = colNamesMore\n",
    "                myData2 = pd.concat([myData2, myData1], axis=1)\n",
    "\n",
    "        myData2['Year'] = year\n",
    "        year = year + 1\n",
    "        \n",
    "        theData = pd.concat([theData, myData2], axis = 0)\n",
    "        \n",
    "    return theData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c27be5-0367-4d72-bff1-a509c101b336",
   "metadata": {},
   "source": [
    "**Function to calcualte heatwaves given the limits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad0ff1-d1fa-4b10-92de-689e438d0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatwaves(limits, temperatures, consecutiveDays, data_loc, tracker = None, verbose = 2):\n",
    "    \n",
    "    HWN = np.zeros([temperatures.shape[0], 1]) # Yearly Number of heat waves\n",
    "    HWN_JFM = np.zeros([temperatures.shape[0], 1]) # Number of heat waves January, February, March\n",
    "    HWN_AMJ = np.zeros([temperatures.shape[0], 1]) # Number of heat waves April, May, June\n",
    "    HWN_JAS = np.zeros([temperatures.shape[0], 1]) # Number of heat waves July,August, September\n",
    "    \n",
    "    HWD = np.zeros([temperatures.shape[0], 1]) # Length of the longest yearly event\n",
    "    \n",
    "    HWF = np.zeros([temperatures.shape[0], 1]) # Yearly sum of participating heat waves\n",
    "    HWF_JFM = np.zeros([temperatures.shape[0], 1]) # Sum of participating heat waves January, February, March\n",
    "    HWF_AMJ = np.zeros([temperatures.shape[0], 1]) # Sum of participating heat waves April, May, June\n",
    "    HWF_JAS = np.zeros([temperatures.shape[0], 1]) # Sum of participating heat waves July,August, September\n",
    "\n",
    "    keepDays = np.zeros([temperatures.shape[0], temperatures.shape[1]]) # To keep track of HWM\n",
    "    keepDays_JFM = np.zeros([temperatures.shape[0], temperatures.shape[1]]) # To keep track of HWM_JFM\n",
    "    keepDays_AMJ = np.zeros([temperatures.shape[0], temperatures.shape[1]]) # To keep track of HWM_AMJ\n",
    "    keepDays_JAS = np.zeros([temperatures.shape[0], temperatures.shape[1]]) # To keep track of HWM_JAS\n",
    "    \n",
    "    for i in range(0, temperatures.shape[0]):\n",
    "        count = 0\n",
    "        if tracker is not None:\n",
    "            if verbose > 0:\n",
    "                print(tracker.iloc[i, -1], tracker.iloc[i, 1])\n",
    "                print('------------------------------------------------------------')\n",
    "            if i == 0:\n",
    "                myfile = open(os.path.join(data_loc, 'Heatwaves', 'trackFile_' + str(consecutiveDays) +'.txt'), 'w')\n",
    "                myfile.write(tracker.iloc[i, -1] + ' ' + str(tracker.iloc[i, 1]) + ': ' + str(temperatures.shape[0]-i)  + ' iterations left\\n')\n",
    "                myfile.close()\n",
    "            else:\n",
    "                myfile = open(os.path.join(data_loc, 'Heatwaves', 'trackFile_' + str(consecutiveDays) +'.txt'), 'a')\n",
    "                myfile.write(tracker.iloc[i, -1] + ' ' + str(tracker.iloc[i, 1]) + ': ' + str(temperatures.shape[0]-i)  + ' iterations left\\n')\n",
    "                myfile.close()\n",
    "                \n",
    "                \n",
    "        for j in range(0,temperatures.shape[1]-1):\n",
    "            if temperatures.iloc[i,j]>limits.iloc[i,j] and temperatures.iloc[i,j+1]>limits.iloc[i,j+1]:\n",
    "                count = count + 1\n",
    "                if temperatures.columns[j+1] == 'Dec31TMax':\n",
    "                    if consecutiveDays < (count+1):\n",
    "                        HWF[i,0] =  HWF[i,0] + count + 1 # For example, if count = 1 then there were 2 consecutive days\n",
    "                        HWN[i,0] = HWN[i,0] + 1\n",
    "                        keepDays[i,j-count:j+2] = 1\n",
    "                        if verbose > 1:\n",
    "                            print('Last day is', temperatures.columns[j+1], '; consecutive days are:', (count+1), \n",
    "                                  '; temperature is: {: .2f}'.format(temperatures.iloc[i,j+1]), \n",
    "                                  '; Limit is: {: .2f}'.format(limits.iloc[i,j+1]),\n",
    "                                 \"({})\".format(limits.columns[j+1]), \"index\", i)\n",
    "\n",
    "                        if HWD[i,0] < (count+1):\n",
    "                            HWD[i,0] = count + 1 # For example, if count = 1 then there were 2 consecutive days  \n",
    "                        count = 0\n",
    "            elif count > 0 and temperatures.iloc[i,j]>limits.iloc[i,j]:\n",
    "                if consecutiveDays < (count+1):\n",
    "                    HWF[i,0] = HWF[i,0] + count + 1 # For example, if count = 1 then there were 2 consecutive days\n",
    "                    HWN[i,0] = HWN[i,0] + 1\n",
    "                    keepDays[i,j-count:j+1] = 1\n",
    "                    if verbose > 1:\n",
    "                        print('Last day is', temperatures.columns[j], '; consecutive days are:', (count+1), \n",
    "                                  '; temperature is: {: .2f}'.format(temperatures.iloc[i,j]), \n",
    "                              '; Limit is: {: .2f}'.format(limits.iloc[i,j]),\n",
    "                                 \"({})\".format(limits.columns[j]), \"index\", i)\n",
    "\n",
    "                    # Assign heatwave to season according to end date\n",
    "\n",
    "                    if temperatures.columns[j][0:3] == \"Jun\" or temperatures.columns[j][0:3] == \"Apr\" or temperatures.columns[j][0:3] == \"May\":\n",
    "                            HWN_AMJ[i,0] = HWN_AMJ[i,0] + 1\n",
    "                            HWF_AMJ[i,0] = HWF_AMJ[i,0] + count + 1\n",
    "                            keepDays_AMJ[i,j-count:j+2] = 1\n",
    "                    elif temperatures.columns[j][0:3] == \"Sep\" or temperatures.columns[j][0:3] == \"Jul\" or temperatures.columns[j][0:3] == \"Aug\":\n",
    "                            HWN_JAS[i,0] = HWN_JAS[i,0] + 1\n",
    "                            HWF_JAS[i,0] = HWF_JAS[i,0] + count + 1\n",
    "                            keepDays_JAS[i,j-count:j+2] = 1\n",
    "                    elif temperatures.columns[j][0:3] == \"Jan\" or temperatures.columns[j][0:3] == \"Feb\" or temperatures.columns[j][0:3] == \"Mar\":\n",
    "                            HWN_JFM[i,0] = HWN_JFM[i,0] + 1\n",
    "                            HWF_JFM[i,0] = HWF_JFM[i,0] + count + 1\n",
    "                            keepDays_JFM[i,j-count:j+2] = 1\n",
    "\n",
    "                    if HWD[i,0] < (count+1):\n",
    "                        HWD[i,0] = count + 1 # For example, if count = 1 then there were 2 consecutive days           \n",
    "                count = 0\n",
    "\n",
    "        if verbose > 1:\n",
    "            print('------------------------------------------------------------')\n",
    "            print('Yearly Number of heat waves are ', HWN[i,0])\n",
    "            print('Number of Heatwaves in JFM ', HWN_JFM[i,0])\n",
    "            print('Number of Heatwaves in AMJ ', HWN_AMJ[i,0])\n",
    "            print('Number of Heatwaves in JAS ', HWN_JAS[i,0])\n",
    "            print('Length of the longest yearly event is ', HWD[i,0])\n",
    "            print('Yearly sum of participating heat waves are ', HWF[i,0])\n",
    "            print('------------------------------------------------------------')\n",
    "            print('')\n",
    "        \n",
    "    a = np.sum(temperatures.values*keepDays, axis=1).reshape(-1,1) # keepdays is 1 only for the days with participating heatwaves\n",
    "    b = np.zeros([temperatures.shape[0], 1])\n",
    "    for i in range(np.shape(a)[0]):\n",
    "        if HWF[i,0] != 0:\n",
    "            b[i,0] = a[i,0]/HWF[i,0]\n",
    "        else: # if no heatwave observed, set the average to the average of max/min temperature of that year, because \"0\" means something else\n",
    "            b[i,0] = np.mean(temperatures.iloc[i, :].values)\n",
    "    TDHW = b\n",
    "    \n",
    "    a = np.sum(temperatures.values*keepDays_JFM, axis=1).reshape(-1,1) # keepdays is 1 only for the days with participating heatwaves\n",
    "    b = np.zeros([temperatures.shape[0], 1])\n",
    "    for i in range(np.shape(a)[0]):\n",
    "        if HWF_JFM[i,0] != 0:\n",
    "            b[i,0] = a[i,0]/HWF_JFM[i,0]\n",
    "        else: # if no heatwave observed, set the average to the average of max/min temperature of that year, because \"0\" means something else\n",
    "            b[i,0] = np.mean(temperatures.iloc[i, 0:90].values) # January 1st is the 1st observation, March 31st is 89th observation\n",
    "    TDHW_JFM = b\n",
    "    \n",
    "    a = np.sum(temperatures.values*keepDays_AMJ, axis=1).reshape(-1,1) # keepdays is 1 only for the days with participating heatwaves\n",
    "    b = np.zeros([temperatures.shape[0], 1])\n",
    "    for i in range(np.shape(a)[0]):\n",
    "        if HWF_AMJ[i,0] != 0:\n",
    "            b[i,0] = a[i,0]/HWF_AMJ[i,0]\n",
    "        else: # if no heatwave observed, set the average to the average of max/min temperature of that year, because \"0\" means something else\n",
    "            b[i,0] = np.mean(temperatures.iloc[i, 90:181].values) # April 1st is the 90th observation, June 30th is 180th observation\n",
    "    TDHW_AMJ = b\n",
    "    \n",
    "    a = np.sum(temperatures.values*keepDays_JAS, axis=1).reshape(-1,1) # keepdays is 1 only for the days with participating heatwaves\n",
    "    b = np.zeros([temperatures.shape[0], 1])\n",
    "    for i in range(np.shape(a)[0]):\n",
    "        if HWF_JAS[i,0] != 0:\n",
    "            b[i,0] = a[i,0]/HWF_JAS[i,0]\n",
    "        else: # if no heatwave observed, set the average to the average of max/min temperature of that year, because \"0\" means something else\n",
    "            b[i,0] = np.mean(temperatures.iloc[i, 181:273].values) # July 1st is the 181st observation, September 30th is 272nd observation\n",
    "    TDHW_JAS = b\n",
    "    \n",
    "    \n",
    "    return HWN, HWN_JFM, HWN_AMJ, HWN_JAS, HWF, HWF_JFM, HWF_AMJ, HWF_JAS, TDHW, TDHW_JFM, TDHW_AMJ, TDHW_JAS, HWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9628403-2efe-46f2-bcfa-9662c94d9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc =  os.path.join(os.path.split(os.path.abspath(os.getcwd()))[0], 'Data') # Adjust the address accordingly\n",
    "print(data_loc)\n",
    "data_loc_temp = os.path.join(data_loc, \"US_County_T\") # Adjust the address accordingly\n",
    "print(data_loc_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea5719-b45b-4912-bbcf-aefe3ac65ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "dummyData = pd.read_csv(os.path.join(data_loc_temp, \"meanDailyTemp1980_1.csv\"))\n",
    "\n",
    "theTemp = 'mean'\n",
    "\n",
    "docName = '/meanDailyTemp'\n",
    "\n",
    "myDataTempMean = genData(dummyData, months, data_loc_temp, docName, theTemp, 'TMean', '_2m_air_temperature')\n",
    "   \n",
    "dummyData = pd.read_csv(os.path.join(data_loc_temp, \"maxDailyTemp1980_1.csv\"))\n",
    "\n",
    "theTemp = 'maximum'\n",
    "\n",
    "docName = '/maxDailyTemp'\n",
    "\n",
    "myDataTempMax = genData(dummyData, months, data_loc_temp, docName, theTemp, 'TMax', '_2m_air_temperature')\n",
    "\n",
    "dummyData = pd.read_csv(os.path.join(data_loc_temp, \"minDailyTemp1980_1.csv\"))\n",
    "\n",
    "theTemp = 'minimum'\n",
    "\n",
    "docName = '/minDailyTemp'\n",
    "\n",
    "myDataTempMin = genData(dummyData, months, data_loc_temp, docName, theTemp, 'TMin', '_2m_air_temperature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd154a1d-c3b5-457e-9338-e3e8fe294564",
   "metadata": {},
   "outputs": [],
   "source": [
    "theTempMean = myDataTempMean.copy()\n",
    "theTempMean = theTempMean.drop(['County','Year', 'StateFP', 'CountyFP'], axis = 1)\n",
    "theTempMean = theTempMean.iloc[0:,0:] - 273.15 # Conver to celsius\n",
    "theTempMean.insert(0,'CountyFP', myDataTempMean['CountyFP'])\n",
    "theTempMean.insert(0, 'StateFP', myDataTempMean['StateFP'])\n",
    "theTempMean.insert(0,'Year', myDataTempMean['Year'])\n",
    "theTempMean.insert(0, 'County', myDataTempMean['County'])\n",
    "theTempMean = theTempMean[theTempMean['Year']<2020]\n",
    "theTempMean = theTempMean[theTempMean['Year']>1978]\n",
    "theTempMean['County_StateFP'] = theTempMean['County'] + ' ' + theTempMean['StateFP'].astype(str)\n",
    "\n",
    "theTempMax = myDataTempMax.copy()\n",
    "theTempMax = theTempMax.drop(['County','Year', 'StateFP', 'CountyFP'], axis = 1)\n",
    "theTempMax = theTempMax.iloc[0:,0:] - 273.15 # Conver to celsius\n",
    "theTempMax.insert(0,'CountyFP', myDataTempMax['CountyFP'])\n",
    "theTempMax.insert(0, 'StateFP', myDataTempMax['StateFP'])\n",
    "theTempMax.insert(0,'Year', myDataTempMax['Year'])\n",
    "theTempMax.insert(0, 'County', myDataTempMax['County'])\n",
    "theTempMax = theTempMax[theTempMax['Year']<2020]\n",
    "theTempMax = theTempMax[theTempMax['Year']>1978]\n",
    "theTempMax['County_StateFP'] = theTempMax['County'] + ' ' + theTempMax['StateFP'].astype(str)\n",
    "\n",
    "theTempMin = myDataTempMin.copy()\n",
    "theTempMin = theTempMin.drop(['County','Year', 'StateFP', 'CountyFP'], axis = 1)\n",
    "theTempMin = theTempMin.iloc[0:,0:] - 273.15 # Conver to celsius\n",
    "theTempMin.insert(0,'CountyFP', myDataTempMin['CountyFP'])\n",
    "theTempMin.insert(0, 'StateFP', myDataTempMin['StateFP'])\n",
    "theTempMin.insert(0,'Year', myDataTempMin['Year'])\n",
    "theTempMin.insert(0, 'County', myDataTempMin['County'])\n",
    "theTempMin = theTempMin[theTempMin['Year']<2020]\n",
    "theTempMin = theTempMin[theTempMin['Year']>1978]\n",
    "theTempMin['County_StateFP'] = theTempMin['County'] + ' ' + theTempMin['StateFP'].astype(str)\n",
    "\n",
    "theTempMax = theTempMax.sort_values(by=['County_StateFP', 'Year'])\n",
    "theTempMin = theTempMin.sort_values(by=['County_StateFP', 'Year'])\n",
    "theTempMean = theTempMean.sort_values(by=['County_StateFP', 'Year'])\n",
    "\n",
    "TempMax = theTempMax.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis = 1)\n",
    "TempMin = theTempMin.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis = 1)\n",
    "TempMean = theTempMean.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfcf40-c6b3-406b-985a-896765318cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "theTempMax_dum = theTempMax.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis = 1)\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec31', theTempMax_dum['Dec31TMax'].shift())\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec30', theTempMax_dum['Dec30TMax'].shift())\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec29', theTempMax_dum['Dec29TMax'].shift())\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec28', theTempMax_dum['Dec28TMax'].shift())\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec27', theTempMax_dum['Dec27TMax'].shift())\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec26', theTempMax_dum['Dec26TMax'].shift())\n",
    "theTempMax_dum.insert(0, 'PreviousYearDec25', theTempMax_dum['Dec25TMax'].shift())\n",
    "theTempMax_dum['NextYearJan1'] = theTempMax_dum['Jan1TMax'].shift(-1)\n",
    "theTempMax_dum['NextYearJan2'] = theTempMax_dum['Jan2TMax'].shift(-1)\n",
    "theTempMax_dum['NextYearJan3'] = theTempMax_dum['Jan3TMax'].shift(-1)\n",
    "theTempMax_dum['NextYearJan4'] = theTempMax_dum['Jan4TMax'].shift(-1)\n",
    "theTempMax_dum['NextYearJan5'] = theTempMax_dum['Jan5TMax'].shift(-1)\n",
    "theTempMax_dum['NextYearJan6'] = theTempMax_dum['Jan6TMax'].shift(-1)\n",
    "theTempMax_dum['NextYearJan7'] = theTempMax_dum['Jan7TMax'].shift(-1)\n",
    "theTempMax_dum.insert(0,'CountyFP', theTempMax['CountyFP'])\n",
    "theTempMax_dum.insert(0, 'StateFP', theTempMax['StateFP'])\n",
    "theTempMax_dum.insert(0,'Year', theTempMax['Year'])\n",
    "theTempMax_dum.insert(0, 'County', theTempMax['County'])\n",
    "theTempMax_dum['County_StateFP'] = theTempMax_dum['County'] + ' ' + theTempMax_dum['StateFP'].astype(str)\n",
    "theTempMax_dum = theTempMax_dum[theTempMax_dum['Year']<2020]\n",
    "theTempMax_dum = theTempMax_dum[theTempMax_dum['Year']>1979]\n",
    "\n",
    "theTempMin_dum = theTempMin.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis = 1)\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec31', theTempMin_dum['Dec31TMin'].shift())\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec30', theTempMin_dum['Dec30TMin'].shift())\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec29', theTempMin_dum['Dec29TMin'].shift())\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec28', theTempMin_dum['Dec28TMin'].shift())\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec27', theTempMin_dum['Dec27TMin'].shift())\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec26', theTempMin_dum['Dec26TMin'].shift())\n",
    "theTempMin_dum.insert(0, 'PreviousYearDec25', theTempMin_dum['Dec25TMin'].shift())\n",
    "theTempMin_dum['NextYearJan1'] = theTempMin_dum['Jan1TMin'].shift(-1)\n",
    "theTempMin_dum['NextYearJan2'] = theTempMin_dum['Jan2TMin'].shift(-1)\n",
    "theTempMin_dum['NextYearJan3'] = theTempMin_dum['Jan3TMin'].shift(-1)\n",
    "theTempMin_dum['NextYearJan4'] = theTempMin_dum['Jan4TMin'].shift(-1)\n",
    "theTempMin_dum['NextYearJan5'] = theTempMin_dum['Jan5TMin'].shift(-1)\n",
    "theTempMin_dum['NextYearJan6'] = theTempMin_dum['Jan6TMin'].shift(-1)\n",
    "theTempMin_dum['NextYearJan7'] = theTempMin_dum['Jan7TMin'].shift(-1)\n",
    "theTempMin_dum.insert(0,'CountyFP', theTempMin['CountyFP'])\n",
    "theTempMin_dum.insert(0, 'StateFP', theTempMin['StateFP'])\n",
    "theTempMin_dum.insert(0,'Year', theTempMin['Year'])\n",
    "theTempMin_dum.insert(0, 'County', theTempMin['County'])\n",
    "theTempMin_dum['County_StateFP'] = theTempMin_dum['County'] + ' ' + theTempMin_dum['StateFP'].astype(str)\n",
    "theTempMin_dum = theTempMin_dum[theTempMin_dum['Year']<2020]\n",
    "theTempMin_dum = theTempMin_dum[theTempMin_dum['Year']>1979]\n",
    "\n",
    "print(theTempMin_dum.shape)\n",
    "iteratorTempMin = theTempMin_dum.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'],axis=1)\n",
    "print(iteratorTempMin.shape)\n",
    "print(theTempMax_dum.shape)\n",
    "iteratorTempMax = theTempMax_dum.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'],axis=1)\n",
    "print(iteratorTempMax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1915d-ac03-4970-b5aa-69ea112aa3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### 90th percentile ####################\n",
    "# 15 day window moving 20-year average tracker\n",
    "Tmin90Perc_15 = pd.DataFrame()\n",
    "Tmax90Perc_15 = pd.DataFrame()\n",
    "# 5 day window moving 20-year average tracker\n",
    "Tmin90Perc_5 = pd.DataFrame()\n",
    "Tmax90Perc_5 = pd.DataFrame()\n",
    "# 15 day window 20-year no moving average tracker\n",
    "Tmin90Perc_15_noMov = pd.DataFrame()\n",
    "Tmax90Perc_15_noMov = pd.DataFrame()\n",
    "# 5 day window 20-year no moving average tracker\n",
    "Tmin90Perc_5_noMov = pd.DataFrame()\n",
    "Tmax90Perc_5_noMov = pd.DataFrame()\n",
    "############### 95th percentile ####################\n",
    "# 15 day window moving 20-year average tracker\n",
    "Tmin95Perc_15 = pd.DataFrame()\n",
    "Tmax95Perc_15 = pd.DataFrame()\n",
    "# 5 day window moving 20-year average tracker\n",
    "Tmin95Perc_5 = pd.DataFrame()\n",
    "Tmax95Perc_5 = pd.DataFrame()\n",
    "# 15 day window 20-year no moving average tracker\n",
    "Tmin95Perc_15_noMov = pd.DataFrame()\n",
    "Tmax95Perc_15_noMov = pd.DataFrame()\n",
    "# 5 day window 20-year no moving average tracker\n",
    "Tmin95Perc_5_noMov = pd.DataFrame()\n",
    "Tmax95Perc_5_noMov = pd.DataFrame()\n",
    "\n",
    "\n",
    "CountyNames = theTempMin_dum['County_StateFP'].unique()\n",
    "\n",
    "num_iter = CountyNames.shape[0]\n",
    "\n",
    "for index, c in enumerate(CountyNames):\n",
    "    print(c, num_iter, 'iterations left')\n",
    "    if index == 0:\n",
    "        myfile = open(os.path.join(data_loc, 'Heatwave_Limits', 'myFile.txt'), 'w')\n",
    "        myfile.write(c + ' ' + str(num_iter) +  ' iterations left\\n')\n",
    "        myfile.close()\n",
    "    else:\n",
    "        myfile = open(os.path.join(data_loc, 'Heatwave_Limits', 'myFile.txt'), \"a\")  # append mode\n",
    "        myfile.write(c + ' ' + str(num_iter) +  ' iterations left\\n')\n",
    "        myfile.close()\n",
    "    \n",
    "    num_iter -= 1    \n",
    "    ############### 90th percentile ####################\n",
    "    # 15 day window moving 20-year \n",
    "    temporary90min_15 = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary90max_15 = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    # 5 day window moving 20-year \n",
    "    temporary90min_5 = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary90max_5 = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    # 15 day window not moving 20-year \n",
    "    temporary90min_15_noMov = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary90max_15_noMov = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    # 5 day window not moving 20-year \n",
    "    temporary90min_5_noMov = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary90max_5_noMov = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    ############### 95th percentile ####################\n",
    "    # 15 day window moving 20-year \n",
    "    temporary95min_15 = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary95max_15 = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    # 5 day window moving 20-year\n",
    "    temporary95min_5 = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary95max_5 = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    # 15 day window not moving 20-year\n",
    "    temporary95min_15_noMov = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary95max_15_noMov = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    # 5 day window not moving 20-year\n",
    "    temporary95min_5_noMov = iteratorTempMin[theTempMin_dum['County_StateFP']== c].copy()\n",
    "    temporary95max_5_noMov = iteratorTempMax[theTempMax_dum['County_StateFP']== c].copy()\n",
    "    \n",
    "    uniqueCountyTmin = iteratorTempMin[theTempMin_dum['County_StateFP']==c].copy()\n",
    "    uniqueCountyTmax = iteratorTempMax[theTempMax_dum['County_StateFP']==c].copy()\n",
    "    for i in range(7, uniqueCountyTmin.shape[1]-7):\n",
    "        for j in range(20,uniqueCountyTmin.shape[0]):\n",
    "            # 90th percentile, 15-day window, 20-year moving average\n",
    "            temporary90min_15.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[(j-20):j,i-7:i+8]),90)\n",
    "            temporary90max_15.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[(j-20):j,i-7:i+8]),90)\n",
    "            # 95th percentile, 15-day window, 20-year moving average\n",
    "            temporary95min_15.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[(j-20):j,i-7:i+8]),95)\n",
    "            temporary95max_15.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[(j-20):j,i-7:i+8]),95)\n",
    "            # 90th percentile, 15-day window, not moving 20-year\n",
    "            temporary90min_15_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[0:20,i-7:i+8]),90)\n",
    "            temporary90max_15_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[0:20,i-7:i+8]),90)\n",
    "            # 95th percentile, 15-day window, not moving 20-year\n",
    "            temporary95min_15_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[0:20,i-7:i+8]),95)\n",
    "            temporary95max_15_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[0:20,i-7:i+8]),95)\n",
    "            \n",
    "            # 90th percentile, 5-day window, 20-year moving average\n",
    "            temporary90min_5.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[(j-20):j,i-7+5:i+8-5]),90)\n",
    "            temporary90max_5.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[(j-20):j,i-7+5:i+8-5]),90)\n",
    "            # 95th percentile, 5-day window, 20-year moving average\n",
    "            temporary95min_5.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[(j-20):j,i-7+5:i+8-5]),95)\n",
    "            temporary95max_5.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[(j-20):j,i-7+5:i+8-5]),95)\n",
    "            # 90th percentile, 5-day window, not moving 20-year\n",
    "            temporary90min_5_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[0:20,i-7+5:i+8-5]),90)\n",
    "            temporary90max_5_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[0:20,i-7+5:i+8-5]),90)\n",
    "            # 95th percentile, 5-day window, not moving 20-year\n",
    "            temporary95min_5_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmin.iloc[0:20,i-7+5:i+8-5]),95)\n",
    "            temporary95max_5_noMov.iloc[j,i] = np.percentile(np.array(uniqueCountyTmax.iloc[0:20,i-7+5:i+8-5]),95)\n",
    "    \n",
    "    ############### 90th percentile ####################    \n",
    "    Tmin90Perc_15 = pd.concat([Tmin90Perc_15, temporary90min_15], axis = 0)\n",
    "    Tmax90Perc_15 = pd.concat([Tmax90Perc_15, temporary90max_15], axis = 0)\n",
    "    Tmin90Perc_5 = pd.concat([Tmin90Perc_5, temporary90min_5], axis = 0)\n",
    "    Tmax90Perc_5 = pd.concat([Tmax90Perc_5, temporary90max_5], axis = 0)\n",
    "    Tmin90Perc_15_noMov = pd.concat([Tmin90Perc_15_noMov, temporary90min_15_noMov], axis = 0)\n",
    "    Tmax90Perc_15_noMov = pd.concat([Tmax90Perc_15_noMov, temporary90max_15_noMov], axis = 0)\n",
    "    Tmin90Perc_5_noMov = pd.concat([Tmin90Perc_5_noMov, temporary90min_5_noMov], axis = 0)\n",
    "    Tmax90Perc_5_noMov = pd.concat([Tmax90Perc_5_noMov, temporary90max_5_noMov], axis = 0)\n",
    "    ############### 95th percentile ####################    \n",
    "    Tmin95Perc_15 = pd.concat([Tmin95Perc_15, temporary95min_15], axis = 0)\n",
    "    Tmax95Perc_15 = pd.concat([Tmax95Perc_15, temporary95max_15], axis = 0)\n",
    "    Tmin95Perc_5 = pd.concat([Tmin95Perc_5, temporary95min_5], axis = 0)\n",
    "    Tmax95Perc_5 = pd.concat([Tmax95Perc_5, temporary95max_5], axis = 0)\n",
    "    Tmin95Perc_15_noMov = pd.concat([Tmin95Perc_15_noMov, temporary95min_15_noMov], axis = 0)\n",
    "    Tmax95Perc_15_noMov = pd.concat([Tmax95Perc_15_noMov, temporary95max_15_noMov], axis = 0)\n",
    "    Tmin95Perc_5_noMov = pd.concat([Tmin95Perc_5_noMov, temporary95min_5_noMov], axis = 0)\n",
    "    Tmax95Perc_5_noMov = pd.concat([Tmax95Perc_5_noMov, temporary95max_5_noMov], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35b204-4762-4b2c-850b-cd6584bce141",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumCols = ['PreviousYearDec25', 'PreviousYearDec26', 'PreviousYearDec27','PreviousYearDec28', \n",
    "           'PreviousYearDec29', 'PreviousYearDec30', 'PreviousYearDec31','NextYearJan1', 'NextYearJan2',\n",
    "           'NextYearJan3','NextYearJan4','NextYearJan5','NextYearJan6', 'NextYearJan7']\n",
    "\n",
    "Tmin90Perc_15 = Tmin90Perc_15.drop(dumCols, axis=1)\n",
    "Tmax90Perc_15 = Tmax90Perc_15.drop(dumCols, axis=1)\n",
    "Tmin90Perc_5 = Tmin90Perc_5.drop(dumCols, axis=1)\n",
    "Tmax90Perc_5 = Tmax90Perc_5.drop(dumCols, axis=1)\n",
    "Tmin90Perc_15_noMov = Tmin90Perc_15_noMov.drop(dumCols, axis=1)\n",
    "Tmax90Perc_15_noMov = Tmax90Perc_15_noMov.drop(dumCols, axis=1)\n",
    "Tmin90Perc_5_noMov = Tmin90Perc_5_noMov.drop(dumCols, axis=1)\n",
    "Tmax90Perc_5_noMov = Tmax90Perc_5_noMov.drop(dumCols, axis=1)\n",
    "\n",
    "Tmin95Perc_15 = Tmin95Perc_15.drop(dumCols, axis=1)\n",
    "Tmax95Perc_15 = Tmax95Perc_15.drop(dumCols, axis=1)\n",
    "Tmin95Perc_5 = Tmin95Perc_5.drop(dumCols, axis=1)\n",
    "Tmax95Perc_5 = Tmax95Perc_5.drop(dumCols, axis=1)\n",
    "Tmin95Perc_15_noMov = Tmin95Perc_15_noMov.drop(dumCols, axis=1)\n",
    "Tmax95Perc_15_noMov = Tmax95Perc_15_noMov.drop(dumCols, axis=1)\n",
    "Tmin95Perc_5_noMov = Tmin95Perc_5_noMov.drop(dumCols, axis=1)\n",
    "Tmax95Perc_5_noMov = Tmax95Perc_5_noMov.drop(dumCols, axis=1)\n",
    "\n",
    "myfile = open(os.path.join(data_loc, 'Heatwave_Limits', 'myFile.txt'), \"a\")  # append mode\n",
    "myfile.write(\"Dummy Columns Dropped \\n\")\n",
    "myfile.close()\n",
    "\n",
    "datasets = [Tmin90Perc_15, Tmax90Perc_15, Tmin90Perc_5, Tmax90Perc_5,\n",
    "           Tmin90Perc_15_noMov, Tmax90Perc_15_noMov, Tmin90Perc_5_noMov, Tmax90Perc_5_noMov,\n",
    "           Tmin95Perc_15, Tmax95Perc_15, Tmin95Perc_5, Tmax95Perc_5,\n",
    "           Tmin95Perc_15_noMov, Tmax95Perc_15_noMov, Tmin95Perc_5_noMov, Tmax95Perc_5_noMov]\n",
    "\n",
    "docNames = ['CTN90pct_15', 'CTX90pct_15', 'CTN90pct_5', 'CTX90pct_5',\n",
    "           'CTN90pct_15_noMov', 'CTX90pct_15_noMov', 'CTN90pct_5_noMov', 'CTX90pct_5_noMov',\n",
    "           'CTN95pct_15', 'CTX95pct_15', 'CTN95pct_5', 'CTX95pct_5',\n",
    "           'CTN95pct_15_noMov', 'CTX95pct_15_noMov', 'CTN95pct_5_noMov', 'CTX95pct_5_noMov']\n",
    "\n",
    "for index, i in enumerate(datasets):\n",
    "    myfile = open(os.path.join(data_loc, 'Heatwave_Limits', 'myFile.txt'), \"a\")  # append mode\n",
    "    myfile.write(docNames[index] + \" going now.\\n\")\n",
    "    myfile.close()\n",
    "    i.insert(0,'County_StateFP',theTempMin_dum['County_StateFP'])\n",
    "    i.insert(0,'CountyFP', theTempMin_dum['CountyFP'])\n",
    "    i.insert(0,'StateFP', theTempMin_dum['StateFP'])\n",
    "    i.insert(0,'Year', theTempMin_dum['Year'])\n",
    "    i.insert(0,'County', theTempMin_dum['County'])\n",
    "    # The first 20 years are used as a baseline\n",
    "    i[i['Year']>1999].to_csv(os.path.join(data_loc, 'Heatwave_Limits', docNames[index] + '.csv'), index=False)\n",
    "\n",
    "myfile = open(os.path.join(data_loc, 'Heatwave_Limits', 'myFile.txt'), \"a\")  # append mode\n",
    "myfile.write(\"\\n DONE \\n\")\n",
    "myfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15aec15-93a1-406e-a434-a6e979a76ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CTN90pct_15 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN90pct_15.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX90pct_15 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX90pct_15.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTN90pct_5 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN90pct_5.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX90pct_5 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX90pct_5.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTN90pct_15_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN90pct_15_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX90pct_15_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX90pct_15_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTN90pct_5_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN90pct_5_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX90pct_5_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX90pct_5_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "\n",
    "CTN95pct_15 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN95pct_15.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX95pct_15 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX95pct_15.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTN95pct_5 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN95pct_5.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX95pct_5 = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX95pct_5.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTN95pct_15_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN95pct_15_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX95pct_15_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX95pct_15_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTN95pct_5_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTN95pct_5_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n",
    "CTX95pct_5_noMov = pd.read_csv(os.path.join(data_loc, 'Heatwave_Limits', 'CTX95pct_5_noMov.csv')).sort_values(by=['County_StateFP', 'Year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070e4fd-d783-49cd-8db1-8d2ebe3e22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datNames = [CTN90pct_15, CTX90pct_15, CTN90pct_5, CTX90pct_5,\n",
    "           CTN90pct_15_noMov, CTX90pct_15_noMov, CTN90pct_5_noMov, CTX90pct_5_noMov,\n",
    "           CTN95pct_15, CTX95pct_15, CTN95pct_5, CTX95pct_5,\n",
    "           CTN95pct_15_noMov, CTX95pct_15_noMov, CTN95pct_5_noMov, CTX95pct_5_noMov]\n",
    "\n",
    "docNames = ['CTN90pct_15', 'CTX90pct_15', 'CTN90pct_5', 'CTX90pct_5',\n",
    "           'CTN90pct_15_noMov', 'CTX90pct_15_noMov', 'CTN90pct_5_noMov', 'CTX90pct_5_noMov',\n",
    "           'CTN95pct_15', 'CTX95pct_15', 'CTN95pct_5', 'CTX95pct_5',\n",
    "           'CTN95pct_15_noMov', 'CTX95pct_15_noMov', 'CTN95pct_5_noMov', 'CTX95pct_5_noMov']\n",
    "\n",
    "print(theTempMin_dum.shape)\n",
    "TempMin = theTempMin_dum[(theTempMin_dum['Year']>1999) & (theTempMin_dum['Year']<2020)]\n",
    "print(TempMin.shape)\n",
    "print(CTN90pct_15.shape)\n",
    "print(theTempMax_dum.shape)\n",
    "TempMax = theTempMax_dum[(theTempMax_dum['Year']>1999) & (theTempMax_dum['Year']<2020)]\n",
    "print(TempMax.shape)\n",
    "print(CTX90pct_15.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c42daa-bf39-4a5a-9582-612a3f4f0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumCols = ['PreviousYearDec25', 'PreviousYearDec26', 'PreviousYearDec27','PreviousYearDec28', \n",
    "           'PreviousYearDec29', 'PreviousYearDec30', 'PreviousYearDec31','NextYearJan1', 'NextYearJan2',\n",
    "           'NextYearJan3','NextYearJan4','NextYearJan5','NextYearJan6', 'NextYearJan7']\n",
    "\n",
    "TempMin = TempMin.drop(dumCols, axis=1)\n",
    "TempMax = TempMax.drop(dumCols, axis=1)\n",
    "\n",
    "TempMin = TempMin.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "TempMax = TempMax.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "\n",
    "CTN90pct_15 = CTN90pct_15.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX90pct_15 = CTX90pct_15.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTN90pct_5 = CTN90pct_5.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX90pct_5 = CTX90pct_5.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTN90pct_15_noMov = CTN90pct_15_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX90pct_15_noMov = CTX90pct_15_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTN90pct_5_noMov = CTN90pct_5_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX90pct_5_noMov = CTX90pct_5_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "\n",
    "CTN95pct_15 = CTN95pct_15.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX95pct_15 = CTX95pct_15.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTN95pct_5 = CTN95pct_5.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX95pct_5 = CTX95pct_5.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTN95pct_15_noMov = CTN95pct_15_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX95pct_15_noMov = CTX95pct_15_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTN95pct_5_noMov = CTN95pct_5_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)\n",
    "CTX95pct_5_noMov = CTX95pct_5_noMov.drop(['County','Year', 'StateFP', 'CountyFP', 'County_StateFP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abf028-e377-45d8-8ede-ad1e70835ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savehws(limits, temperatures, consecutiveDays, data_loc, docName, tracker = None, verbose = 0):\n",
    "    hws = pd.DataFrame()\n",
    "    hws['County'] = theTracker['County'].copy()\n",
    "    hws['Year'] = theTracker['Year'].copy()\n",
    "    hws['StateFP'] = theTracker['StateFP'].copy()\n",
    "    hws['CountyFP'] = theTracker['CountyFP'].copy()\n",
    "    hws['County_StateFP'] = theTracker['County_StateFP'].copy()\n",
    "\n",
    "    HWN, HWN_JFM, HWN_AMJ, HWN_JAS, HWF, HWF_JFM, HWF_AMJ, HWF_JAS, TDHW, TDHW_JFM, TDHW_AMJ, TDHW_JAS, HWD = heatwaves(limits, temperatures, \n",
    "                                                                                                                        consecutiveDays, data_loc, \n",
    "                                                                                                                        tracker, verbose)\n",
    "    hws['# of HW'] = HWN\n",
    "    hws['# of HW JFM'] = HWN_JFM\n",
    "    hws['# of HW AMJ'] = HWN_AMJ\n",
    "    hws['# of HW JAS'] = HWN_JAS\n",
    "    hws['# of HW days'] = HWF\n",
    "    hws['# of HW days JFM'] = HWF_JFM\n",
    "    hws['# of HW days AMJ'] = HWF_AMJ\n",
    "    hws['# of HW days JAS'] = HWF_JAS\n",
    "    hws['T of HW'] = TDHW\n",
    "    hws['T of HW JFM'] = TDHW_JFM\n",
    "    hws['T of HW AMJ'] = TDHW_AMJ\n",
    "    hws['T of HW JAS'] = TDHW_JAS\n",
    "    hws['Longest HW'] = HWD\n",
    "    \n",
    "    try:\n",
    "        hws.to_csv(os.path.join(data_loc, 'Heatwaves', 'HW_from_' + docName + '_' + str(consecutiveDays) + '.csv'), index = False)\n",
    "    except Exception as e:\n",
    "        myfile = open(os.path.join(data_loc, 'Heatwaves', 'errorFile_' + str(consecutiveDays) +'.txt'), 'w')\n",
    "        myfile.write(e)\n",
    "        myfile.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa4ebd-eb68-4d43-a8b1-d76b29e4a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "theTracker = theTempMax_dum[theTempMax_dum['Year']>1999] # To keep track of the county - year pair in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d501f98-8fe3-4dc7-ba7a-bae10c38907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN90pct_15, TempMin, days, data_loc, 'CTN90pct_15', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX90pct_15, TempMax, days, data_loc, 'CTX90pct_15', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN90pct_15_noMov, TempMin, days, data_loc, 'CTN90pct_15_noMov', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX90pct_15_noMov, TempMax, days, data_loc, 'CTX90pct_15_noMov', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN90pct_5, TempMin, days, data_loc, 'CTN90pct_5', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX90pct_5, TempMax, days, data_loc, 'CTX90pct_5', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN90pct_5_noMov, TempMin, days, data_loc, 'CTN90pct_5_noMov', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX90pct_5_noMov, TempMax, days, data_loc, 'CTX90pct_5_noMov', theTracker) for days in np.array([3,6]))\n",
    "\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN95pct_15, TempMin, days, data_loc, 'CTN95pct_15', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX95pct_15, TempMax, days, data_loc, 'CTX95pct_15', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN95pct_15_noMov, TempMin, days, data_loc, 'CTN95pct_15_noMov', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX95pct_15_noMov, TempMax, days, data_loc, 'CTX95pct_15_noMov', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN95pct_5, TempMin, days, data_loc, 'CTN95pct_5', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX95pct_5, TempMax, days, data_loc, 'CTX95pct_5', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTN95pct_5_noMov, TempMin, days, data_loc, 'CTN95pct_5_noMov', theTracker) for days in np.array([3,6]))\n",
    "results = Parallel(n_jobs=2)(delayed(savehws)(CTX95pct_5_noMov, TempMax, days, data_loc, 'CTX95pct_5_noMov', theTracker) for days in np.array([3,6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e42860-7844-4e4f-b950-3b5e93cebffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for index_i, i in enumerate(docNames):\n",
    "    for index_j, j in enumerate(np.array([3,6])):\n",
    "        if index_i == 0 and index_j == 0:\n",
    "            weather = pd.read_csv(os.path.join(data_loc, 'Heatwaves', \n",
    "                                               'HW_from_' + i + '_' + str(j) + '.csv'))\n",
    "            \n",
    "            ## We will use average temperatures of heatwaves for summary statistics, and we only need the selected group\n",
    "            ## hence they will be dopped at this stage            \n",
    "            \n",
    "            weather = weather[weather.columns.drop(list(weather.filter(regex='T of HW')))]\n",
    "            #specify columns to add suffix to\n",
    "            cols = weather.columns[5:]\n",
    "            #add suffix to specific columns\n",
    "            weather = weather.rename(columns={c: c+ ' (' + i[2:5] + '-' + i[-2:] + '-' + str(j) + ')' for c in weather.columns if c in cols})\n",
    "            group_index = np.ones(weather.columns[5:].shape).astype(int) # needed for Sparse Group LASSO\n",
    "        else:\n",
    "            temp = pd.read_csv(os.path.join(data_loc, 'Heatwaves', \n",
    "                                               'HW_from_' + i + '_' + str(j) + '.csv'))\n",
    "            \n",
    "            temp = temp[temp.columns.drop(list(temp.filter(regex='T of HW')))]\n",
    "            \n",
    "            cols = temp.columns[5:]\n",
    "            \n",
    "            if \"noMov\" in i and \"15\" in i:\n",
    "                temp = temp.rename(columns={c: c+ ' (' + i[2:5] + '-' + i[9:11] + '-' + str(j) + '-NM)' for c in temp.columns if c in cols})\n",
    "\n",
    "            elif \"noMov\" in i and \"15\" not in i:   \n",
    "                temp = temp.rename(columns={c: c+ ' (' + i[2:5] + '-' + i[9:10] + '-' + str(j) + '-NM)' for c in temp.columns if c in cols})             \n",
    "            elif \"noMov\" not in i and \"15\" not in i:\n",
    "                temp = temp.rename(columns={c: c+ ' (' + i[2:5] + '-' + i[-1] + '-' + str(j) + ')' for c in temp.columns if c in cols})\n",
    "\n",
    "            else:\n",
    "                temp = temp.rename(columns={c: c+ ' (' + i[2:5] + '-' + i[-2:] + '-' + str(j) + ')' for c in temp.columns if c in cols})\n",
    "            \n",
    "            weather = pd.merge(weather, temp, left_on = ['County', 'Year', 'StateFP', 'CountyFP', 'County_StateFP'],\n",
    "                              right_on = ['County', 'Year', 'StateFP', 'CountyFP', 'County_StateFP'])\n",
    "            \n",
    "            group_index = np.concatenate((group_index, np.ones(weather.columns[5:].shape).astype(int) + index))\n",
    "        \n",
    "        index = index + 1\n",
    "            \n",
    "    \n",
    "        \n",
    "weather.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f56f60-2209-45b6-abe5-452a8bf03b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_csv(os.path.join(data_loc, 'WEATHER.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bf263-ef63-4fa2-b915-c40a58850212",
   "metadata": {},
   "source": [
    "# MERGING WEATHER DATA WITH ECON DATA AND CONDUCTING LASSO - GROUP LASSO - SPARSE GROUP LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f2ffd-2745-4ae8-a4cb-9e2e6336c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.matlib\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Lars\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "import time\n",
    "\n",
    "import asgl\n",
    "\n",
    "# Check software versions\n",
    "print('### Python version: ' + __import__('sys').version) # 3.9.12\n",
    "print('### NumPy version: ' + np.__version__) # 1.22.3\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2dda-e5f9-4575-bdc5-56db1f3c1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "parentPath = os.path.split(os.path.abspath(os.getcwd()))[0]\n",
    "print(parentPath)\n",
    "weather = pd.read_csv(os.path.join(parentPath, 'Data', 'WEATHER.csv'))\n",
    "weather.head() # Each group has 9 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713453e-1006-4ac7-986d-a3f5a6860042",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = weather[['County', 'StateFP', 'CountyFP']]\n",
    "data = data.drop_duplicates(subset = ['County', 'StateFP', 'CountyFP'])\n",
    "data['GeoFIPS'] = ''\n",
    "for index, row in data.iterrows():\n",
    "    if row['StateFP'] < 10:\n",
    "        if row['CountyFP'] < 10:\n",
    "            data.loc[index, 'GeoFIPS'] = '0' + str(row['StateFP']) + '00' + str(row['CountyFP'])\n",
    "        elif row['CountyFP'] > 9 and row['CountyFP'] < 100:\n",
    "            data.loc[index, 'GeoFIPS'] = '0' + str(row['StateFP']) + '0' + str(row['CountyFP'])\n",
    "        elif row['CountyFP'] > 99:\n",
    "            data.loc[index, 'GeoFIPS'] = '0' + str(row['StateFP']) + str(row['CountyFP'])\n",
    "    if row['StateFP'] > 9:\n",
    "        if row['CountyFP'] < 10:\n",
    "            data.loc[index, 'GeoFIPS'] = str(row['StateFP']) + '00' + str(row['CountyFP'])\n",
    "        elif row['CountyFP'] > 9 and row['CountyFP'] < 100:\n",
    "            data.loc[index, 'GeoFIPS'] = str(row['StateFP']) + '0' + str(row['CountyFP'])\n",
    "        elif row['CountyFP'] > 99:\n",
    "            data.loc[index, 'GeoFIPS'] = str(row['StateFP']) + str(row['CountyFP'])\n",
    "            \n",
    "data.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4202a4-7464-48fa-932d-ad2fd055c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather.shape)\n",
    "weather = pd.merge(weather, data)\n",
    "print(weather.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc179a29-b7ad-41e3-aa1a-a147456f8474",
   "metadata": {},
   "source": [
    "## Heatwave Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b828a8-09e2-41a9-827c-35fe667c66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# map coloring\n",
    "from matplotlib import cm\n",
    "import cmasher as cmr\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "cols = np.array(['# of HW (X95-15-6)', '# of HW (X95-15-6-NM)', '# of HW (X95-5-6)', '# of HW (X90-15-6)',\n",
    "                '# of HW (X95-5-3)', '# of HW (X95-5-3-NM)', '# of HW (X95-15-3)', '# of HW (X90-5-3)'])\n",
    "\n",
    "\n",
    "\n",
    "for col in cols:\n",
    "    df_sample = theData[theData['Year']==2012]\n",
    "    df_sample = df_sample[['GeoFIPS', col]]\n",
    "\n",
    "    colorscale = cmr.take_cmap_colors('Reds', 12, return_fmt='hex')\n",
    "\n",
    "    endpts = list(np.linspace(0, np.max(theData['# of HW (X90-5-3)']), len(colorscale) - 1))\n",
    "    fips = df_sample['GeoFIPS'].tolist()\n",
    "    values = df_sample[col].tolist()\n",
    "\n",
    "\n",
    "    fig = ff.create_choropleth(\n",
    "        fips=fips, values=values, scope=['usa'],\n",
    "        binning_endpoints=endpts, colorscale=colorscale,\n",
    "        show_state_data=True,\n",
    "        state_outline = {'color': 'gray', 'width': .5},\n",
    "        show_hover=True,\n",
    "        round_legend_values = True,\n",
    "        asp = 2.9,\n",
    "       # title_text = col.replace('#', 'Number') + \" in \" + str(2012),\n",
    "        legend_title = col \n",
    "    )\n",
    "    fig.layout.template = None\n",
    "\n",
    "    fig.write_image(os.path.join(parentPath, 'Figures', col[9:-1] + \".png\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7c27c-3570-41b5-89fe-3171765abf45",
   "metadata": {},
   "source": [
    "### Download county level gdp from https://apps.bea.gov/regional/downloadzip.cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b9568-0dbd-4664-8b25-5662585525e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(parentPath, 'Data', 'CAINC1__ALL_AREAS_1969_2021.csv'))\n",
    "data = data[data['Description'] != 'Personal income (thousands of dollars) ']\n",
    "data.loc[data['Description'] == 'Population (persons) 1/', 'Description'] = 'Population'\n",
    "data.loc[data['Description'] == 'Per capita personal income (dollars) 2/', 'Description'] = 'Per capita personal income'\n",
    "data.loc[:,'GeoFIPS'] = data.loc[:,'GeoFIPS'].apply(lambda s: s[2:-1]) # Erase \" \"\n",
    "data = data[data['Region'] != ' '] # No need for US level data now\n",
    "# No information for IndustryClassification, only '...'\n",
    "data = data.drop(['TableName', 'LineCode', 'IndustryClassification', 'Unit'], axis=1) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e88786-7276-4ab4-9058-5cd06e763fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_temp = data[data['Description'] == 'Population']\n",
    "population_temp = population_temp.drop('Description', axis =1)\n",
    "income_temp = data[data['Description'] == 'Per capita personal income']\n",
    "income_temp = income_temp.drop('Description', axis =1)\n",
    "\n",
    "cols = population_temp.columns[3:]\n",
    "for index, i in enumerate(cols):\n",
    "    if index == 0:\n",
    "        # Population\n",
    "        population = population_temp[['GeoFIPS', 'GeoName', 'Region', i]].copy()\n",
    "        population['Year'] = int(i)\n",
    "        population = population.rename(columns = {i: 'Population'})\n",
    "        # Income\n",
    "        income = income_temp[['GeoFIPS', 'GeoName', i]].copy()\n",
    "        income['Year'] = int(i)\n",
    "        income = income.rename(columns = {i: 'Personalincome_pc'})\n",
    "    else:\n",
    "        # Population\n",
    "        temp = population_temp[['GeoFIPS', 'GeoName', 'Region', i]].copy()\n",
    "        temp['Year'] = int(i)\n",
    "        temp = temp.rename(columns = {i: 'Population'})\n",
    "        population = pd.concat((population, temp), axis=0)\n",
    "        # income\n",
    "        temp = income_temp[['GeoFIPS', 'GeoName', i]].copy()\n",
    "        temp['Year'] = int(i)\n",
    "        temp = temp.rename(columns = {i: 'Personalincome_pc'})\n",
    "        income = pd.concat((income, temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450b39a-0578-4edc-bf74-0d8e1a290cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bea = pd.merge(population, income)\n",
    "print('Shape of population data: {}, shape of income data: {}, shape of merged data: {}'.format(population.shape[0],\n",
    "                                                                                                income.shape[0],\n",
    "                                                                                                bea.shape[0]))\n",
    "bea = bea[bea['Population'] != \"(NA)\"]\n",
    "bea['Population'] = bea['Population'].astype(float)\n",
    "bea['Personalincome_pc'] = bea['Personalincome_pc'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100ec5c-3e93-4c13-9265-5a2f1cc9ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cpi = pd.read_csv(os.path.join(parentPath, \"Data\", \"USACPIALLAINMEI.csv\"))\n",
    "data_cpi['Year'] = np.arange(1960, 2023, 1)\n",
    "data_cpi = data_cpi.drop('DATE', axis=1)\n",
    "data_cpi = data_cpi.rename(columns = {'USACPIALLAINMEI': 'CPI'})\n",
    "data_cpi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb35d09-a9f6-4d13-8291-528eeacd14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bea.shape)\n",
    "bea = pd.merge(bea, data_cpi)\n",
    "print(bea.shape)\n",
    "bea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b8f77-5713-4908-a44d-5bfbd77e9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bea.columns[5:-1]:\n",
    "    bea[i] = 100*bea[i]/bea['CPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8721b7-9c55-4cae-a422-bea49bc7b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take logs\n",
    "bea[bea.columns[5:-1]] =  np.log(1000*bea[bea.columns[5:-1]]) # Data is in thousands\n",
    "bea = bea.sort_values(by = ['GeoFIPS', 'Year'])\n",
    "\n",
    "potential_LHS = []\n",
    "for i in bea.columns[5:-1]:\n",
    "    bea['L_' + i] = bea.groupby('GeoFIPS')[i].shift()\n",
    "    bea['D_' + i] = bea[i] - bea['L_' + i]\n",
    "    potential_LHS.append('D_' + i)\n",
    "    bea = bea.drop([i, 'L_' + i], axis = 1)\n",
    "bea = bea.sort_values(by = ['GeoFIPS', 'Year'])\n",
    "potential_LHS = np.array(potential_LHS)\n",
    "\n",
    "# Add lags for the dependent variable\n",
    "lags = 2\n",
    "lags_RHS = []\n",
    "for j in potential_LHS:\n",
    "    for i in range(1,lags+1):\n",
    "        bea['L' + str(i) + '_' + j] = bea.groupby('GeoFIPS')[j].shift(i)\n",
    "        lags_RHS.append('L' + str(i) + '_' + j)\n",
    "bea = bea.dropna(subset = lags_RHS)\n",
    "bea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0127107-ad2f-4c3a-b8e8-7c948078263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop States outside of US mainland\n",
    "weather = weather[weather['StateFP']<57]\n",
    "# Drop Hawaii\n",
    "weather = weather[weather['StateFP']!=15]\n",
    "# Drop Alaska\n",
    "weather = weather[weather['StateFP']!=2]\n",
    "# Drop DC\n",
    "weather = weather[weather['StateFP']!=11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7c94c-12bc-4e86-92dc-16259007b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = pd.merge(bea, weather, left_on = ['GeoFIPS', 'Year'], right_on = ['GeoFIPS', 'Year'])\n",
    "mergedData.insert(3, 'year', mergedData['Year'])\n",
    "mergedData = mergedData.drop('Year', axis = 1)\n",
    "mergedData = mergedData.rename(columns = {'year': 'Year'})\n",
    "print('Shape of bea data: {}, shape of weather data: {}, shape of merged data: {}'.format(bea.shape[0],\n",
    "                                                                                                weather.shape[0],\n",
    "                                                                                                mergedData.shape[0]))\n",
    "mergedData = mergedData.drop(['CPI'], axis = 1)\n",
    "mergedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5af2c5-8b3d-43b2-9cad-136b8dac051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isin(np.unique(weather['GeoFIPS']), np.unique(mergedData['GeoFIPS']), invert = True)\n",
    "print(np.unique(weather['GeoFIPS'])[mask])\n",
    "# Virginia is 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51a948-c1d4-4a99-808c-50161e909bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = mergedData.drop(['Region', 'Population'], axis=1)\n",
    "allData = allData.sort_values(by = ['GeoFIPS', 'Year'])\n",
    "weatherCols = allData.columns[19:]\n",
    "\n",
    "weather_events = []\n",
    "for i in weatherCols:\n",
    "    allData['D_' + i] = allData.groupby('GeoFIPS')[i].diff()\n",
    "\n",
    "    allData = allData.drop([i], axis=1)\n",
    "     \n",
    "    weather_events.append('D_' + i)\n",
    "\n",
    "    \n",
    "allData = allData.sort_values(by = ['GeoFIPS', 'Year'])\n",
    "allData = allData.dropna(subset = np.array(weather_events))\n",
    "allData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb98c48-cd43-4633-baa5-583ab99b5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "potentialLHS = np.array(potential_LHS)\n",
    "lagsRHS = np.array(lags_RHS)\n",
    "weatherEvents = np.array(weather_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21230a2e-cfda-40eb-a33a-06806d020d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent = 'Personalincome_pc'\n",
    "lags_dependent = lagsRHS[np.flatnonzero(np.core.defchararray.find(lagsRHS, 'D_' + dependent)!=-1)]\n",
    "#allRHS = np.concatenate((lags_dependent, np.array(['Government_Farm_State'])))\n",
    "allRHS = np.concatenate((lags_dependent, weatherEvents))\n",
    "\n",
    "allData_ = allData.copy()\n",
    "allData_[np.concatenate((np.array(['D_' + dependent]), lags_dependent))].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd881b86-c9cb-4217-9c5e-f09a9bddc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "i = 2019\n",
    "col = 'D_Personalincome_pc'\n",
    "df_sample = allData_[allData_['Year']==i]\n",
    "df_sample = df_sample[['GeoFIPS', col]]\n",
    "\n",
    "colorscale = [\"#750e13\", \"#a2191f\", \"#da1e28\", \"#fa4d56\", \"#ff8389\", \"#ffb3b8\", \"#ffd7d9\",\n",
    "        \"#deebf7\", \"#c6dbef\", \"#9ecae1\", \"#6baed6\", \"#4292c6\",\n",
    "        \"#2171b5\", \"#08519c\", \"#08306b\"\n",
    "    ]\n",
    "\n",
    "endpts = list(np.linspace(np.min(df_sample[col]), np.max(df_sample[col]), len(colorscale) - 1))\n",
    "fips = df_sample['GeoFIPS'].tolist()\n",
    "values = df_sample[col].tolist()\n",
    "\n",
    "\n",
    "fig = ff.create_choropleth(\n",
    "    fips=fips, values=values, scope=['usa'],\n",
    "    binning_endpoints=endpts, colorscale=colorscale,\n",
    "    show_state_data=True,\n",
    "    state_outline = {'color': 'gray', 'width': .5},\n",
    "    show_hover=True,\n",
    "    round_legend_values = False,\n",
    "    asp = 2.9,\n",
    "    #title_text = 'Personal Income Growth ' + str(i-1) + '-' + str(i),\n",
    "    #legend_title = r'$\\Delta$' + 'log(personal income per capita)' \n",
    ")\n",
    "fig.layout.template = None\n",
    "\n",
    "fig.write_image(col + '_' + str(i) + \".png\")\n",
    "\n",
    "fig.write_image(os.path.join(parentPath, 'Figures', col + '_' + str(i) + \".png\"))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e807fc-faf2-43f1-92bd-16f970d27648",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_limit_up = allData_['D_' + dependent].quantile(0.99)\n",
    "dependent_limit_down = allData_['D_' + dependent].quantile(0.01)\n",
    "\n",
    "\n",
    "allData_ = allData_[(allData_['D_' + dependent]>=dependent_limit_down) & (allData_['D_' + dependent]<=dependent_limit_up) &\n",
    "                   (allData_[lags_dependent[0]] >= dependent_limit_down) & (allData_[lags_dependent[0]] <= dependent_limit_up) &\n",
    "                   (allData_[lags_dependent[1]] >= dependent_limit_down) & (allData_[lags_dependent[1]] <= dependent_limit_up)]\n",
    "\n",
    "allData_[np.concatenate((np.array(['D_' + dependent]), lags_dependent))].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2defa82-3654-4b52-91e4-ccaed6c22305",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "i = 2019\n",
    "col = 'D_Personalincome_pc'\n",
    "df_sample = allData_[allData_['Year']==i]\n",
    "df_sample = df_sample[['GeoFIPS', col]]\n",
    "\n",
    "colorscale = [\"#750e13\", \"#a2191f\", \"#da1e28\", \"#fa4d56\", \"#ff8389\", \"#ffb3b8\", \"#ffd7d9\",\n",
    "        \"#deebf7\", \"#c6dbef\", \"#9ecae1\", \"#6baed6\", \"#4292c6\",\n",
    "        \"#2171b5\", \"#08519c\", \"#08306b\"\n",
    "    ]\n",
    "\n",
    "endpts = list(np.linspace(np.min(df_sample[col]), np.max(df_sample[col]), len(colorscale) - 1))\n",
    "fips = df_sample['GeoFIPS'].tolist()\n",
    "values = df_sample[col].tolist()\n",
    "\n",
    "\n",
    "fig = ff.create_choropleth(\n",
    "    fips=fips, values=values, scope=['usa'],\n",
    "    binning_endpoints=endpts, colorscale=colorscale,\n",
    "    show_state_data=True,\n",
    "    state_outline = {'color': 'gray', 'width': .5},\n",
    "    show_hover=True,\n",
    "    round_legend_values = False,\n",
    "    asp = 2.9,\n",
    "    #title_text = 'log() ' + str(i),\n",
    "    # legend_title = col \n",
    ")\n",
    "fig.layout.template = None\n",
    "\n",
    "fig.write_image(os.path.join(parentPath, 'Figures', col + '_trimmed_' + str(i) + \".png\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d993b-36e9-4c76-b40d-65197e56e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "yOriginal = allData_['D_' + dependent].values\n",
    "XOriginal = allData_[allRHS].values\n",
    "X = preprocessing.StandardScaler().fit_transform(XOriginal)\n",
    "y = preprocessing.StandardScaler().fit_transform(yOriginal.reshape(-1,1))\n",
    "y = y.reshape(-1,)\n",
    "regData = pd.DataFrame()\n",
    "regData['GeoFIPS']= allData_['GeoFIPS'].copy()\n",
    "regData['StateName']= allData_['StateName'].copy()\n",
    "regData['Year']= allData_['Year'].copy()\n",
    "regData['ClimateRegion'] = allData_['ClimateRegion'].copy()\n",
    "regData[allRHS] = X.copy()\n",
    "regData['D_' + dependent] = y.copy()\n",
    "\n",
    "regData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dddc92-cb8e-4f9c-8821-20c90b523eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year Dummies\n",
    "yearDummies = pd.get_dummies(regData['Year'], prefix='y', dtype = 'float32')\n",
    "# County Dummies\n",
    "countyDummies = pd.get_dummies(regData['GeoFIPS'], prefix = 's', dtype = 'float32')\n",
    "# Generate Matrix with both Year and Country Dummies\n",
    "theDummies = pd.concat((yearDummies, countyDummies), axis = 1)\n",
    "print('Year Fixed Effects and County Fixed Effects')\n",
    "theDummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96883a-7985-4475-860c-c8b1d642cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcedRHS = theDummies.drop(['s_56045'], axis=1).values # drop one of the columns because of the intercept\n",
    "squareMatrix = np.transpose(forcedRHS)@forcedRHS\n",
    "\n",
    "y_noTDummy = y - forcedRHS@(np.linalg.inv(squareMatrix) @ (np.transpose(forcedRHS)@y))\n",
    "X_noTDummy = X - forcedRHS@(np.linalg.inv(squareMatrix) @ (np.transpose(forcedRHS)@X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212026f5-634a-46d1-a790-437222dce271",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_index = np.ones(2)\n",
    "for i in range(1, int((X_noTDummy.shape[1]-2)/9)+1): # the first two columns are lags, there are 9 features by group\n",
    "    temp = np.ones(9) + i\n",
    "    group_index = np.concatenate((group_index, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80c5b6-41b8-42d5-9e81-a22a7e3b5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_model = asgl.ASGL(model='lm', penalization='gl', lambda1=.034)\n",
    "gl_model.fit(x=X_noTDummy,y=y_noTDummy, group_index=group_index)\n",
    "nonZero_gl = np.nonzero(gl_model.coef_[0])[0]-1 # the first one is the intercept\n",
    "print('The variables selected by Group LASSO are: ')\n",
    "print(str(allRHS[nonZero_gl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586de4b5-4930-4a34-8933-6d5ff95fa273",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgl_model = asgl.ASGL(model='lm', penalization='sgl', lambda1=.0355, alpha=.465)  \n",
    "sgl_model.fit(x=X_noTDummy,y=y_noTDummy, group_index=group_index)\n",
    "nonZero_sgl = np.nonzero(sgl_model.coef_[0])[0]-1 # the first one is the intercept\n",
    "print('The variables selected by Sparse Group LASSO are: ')\n",
    "print(str(allRHS[nonZero_sgl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de8f42-1fd3-46fd-ab71-be42fc0417b2",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9d221-a8b9-4180-8b3d-370cffaaae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearmodels.panel import PanelOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ab540-f609-43ee-891f-7c56c0f93c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "regData_= allData_.copy()\n",
    "regData_[np.concatenate((np.array('D_' + dependent).reshape(1,), \n",
    "                          lags_dependent))] = regData_[np.concatenate((np.array('D_' + dependent).reshape(1,), \n",
    "                                                                         lags_dependent))]*100 # convert to percentages\n",
    "regData_ = regData_.set_index(['GeoFIPS','Year'])\n",
    "\n",
    "# exog_vars = np.concatenate((lags_dependent, np.array(['D_PDSI_moreThan4','D_PDSI_lessThan_4',\n",
    "#  'D_DTR', 'D_TNHW'])))\n",
    "exog_vars = allRHS[nonZero_sgl]\n",
    "\n",
    "\n",
    "exog = regData_[exog_vars]\n",
    "# exog = exog.rename()\n",
    "indep = regData_['D_' + dependent]\n",
    "\n",
    "# Run the regressions\n",
    "mod = PanelOLS(indep, exog, entity_effects = True, time_effects = True)\n",
    "res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
